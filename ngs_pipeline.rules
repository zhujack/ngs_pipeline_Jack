import itertools
import os
import collections
import json
from snakemake.utils import R
from snakemake.exceptions import MissingInputException
# Snakemake Base location
try:
    NGS_PIPELINE=os.environ['NGS_PIPELINE']
    WORK_DIR=os.environ['WORK_DIR']
    DATA_DIR=os.environ['DATA_DIR']
    DATA_DIR_fastq=os.environ['DATA_DIR_fastq']
    ACT_DIR=os.environ['ACT_DIR']
except KeyError:
    NGS_PIPELINE="/data/Clinomics/Tools/ngs-pipeline"
    ACT_DIR="/Actionable/"
    pass
shell.prefix("set -eo pipefail; ")
configfile: NGS_PIPELINE +"/config_common.json"
configfile: NGS_PIPELINE +"/config_cluster.json"
configfile: "samplesheet.json" ## Project specific

###########################################################################
#
#            Conversion
#
###########################################################################
SUBJECT_TO_SAMPLE  = {}
for subject in config['subject']:
    SUBJECT_TO_SAMPLE[subject] = expand("{sample}", sample = config['subject'][subject])
###########################################################################
SAMPLE_TO_SUBJECT  = {}
for subject,samples in config['subject'].items():
    for sample in samples:
        SAMPLE_TO_SUBJECT[sample]=subject
###########################################################################
## chromesomes
CHR = list(range(1,23)) + ["X", "Y"]
chrList = [ "chr" + str(c) for c in CHR ]

####
#### Targets
####
PATIENTS =[]
SUBS  = []
SUB_BAMS= {}
SUB_COV = {}
SUB_LOH = {}
SUB_GT  = {}
SUB_MPG = {}
SUB_HOT = {}
SUB_IGV = {}
SAMPLES =[]
somaticPairs = {}
somaticCopy = {}
pairedCapture = {}
for subject in config['subject'].keys():
    SUBS.append(subject)
    PATIENTS.append(subject)
    SUB_BAMS[subject]= ["{subject}/{sample}/{sample}.bwa.final.bam".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][subject]]
    SUB_COV[subject] = ["{subject}/{sample}/qc/{sample}.bwa.coverage.txt".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][subject]]
    SUB_HOT[subject] = ["{subject}/{sample}/qc/{sample}.bwa.hotspot.depth".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][subject]]
    SUB_LOH[subject] = ["{subject}/{sample}/qc/{sample}.bwa.loh".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][subject]]
    SUB_GT[subject]  = ["{subject}/{sample}/qc/{sample}.bwa.gt".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][subject]]
    SUB_MPG[subject] = ["{subject}/{sample}/calls/{sample}.bam2mpg.vcf.gz".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][subject]]
    SUB_IGV[subject] = ["{subject}/{sample}/{sample}.bwa.final.bam".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][subject]]
    SUB_IGV[subject] += ["{subject}/{sample}/{sample}.bwa.final.bam.tdf".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][subject]]
    SUB_IGV[subject] += ["{subject}/{sample}/{sample}.novo.final.bam".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][subject]] 
    SUB_IGV[subject] += ["{subject}/{sample}/{sample}.novo.final.bam.tdf".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][subject]] 
  
    for sample in config['subject'][subject]:
        SAMPLES.append(sample)
###########################################################################
#        Add RNASeq only samples to PATIENTS
###########################################################################
for subject  in config['RNASeq'].keys():
        if subject not in PATIENTS:
                PATIENTS.append(subject)    
###########################################################################
ALL_FASTQ1  = expand(DATA_DIR + "/{sample}_R1.fastq.gz", sample=config["sample_fastq1"].keys())
ALL_FASTQ2  = expand(DATA_DIR + "/{sample}_R2.fastq.gz", sample=config["sample_fastq2"].keys())
ALL_FASTQ = ALL_FASTQ1 + ALL_FASTQ2
ALL_FASTQC  = ["{subject}/{sample}/qc/fastqc/{sample}_R2_fastqc.html".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
ALL_BAMS    = ["{subject}/{sample}/{sample}.bwa.final.bam".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
ALL_BAMS   += ["{subject}/{sample}/{sample}.novo.final.bam".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
ALL_QC      = ["{subject}/{sample}/qc/{sample}.bwa.flagstat.txt".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES] 
ALL_QC     += ["{subject}/{sample}/qc/{sample}.bwa.hotspot.depth".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES] 
ALL_QC     += ["{subject}/{sample}/qc/{sample}.bwa.gt".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES] 
# ALL_QC     += ["{subject}/{sample}/qc/BamQC/qualimapReport.html".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
ALL_QC     += ["{subject}/{sample}/copyNumber/{sample}.count.txt".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
ALL_QC     += expand("{subject}/qc/{subject}.genotyping.txt", subject=PATIENTS)
ALL_QC     += expand("{subject}/qc/{subject}.coveragePlot.png", subject=PATIENTS)
ALL_QC     += expand("{subject}/qc/{subject}.circos.png", subject=PATIENTS)
ALL_QC     += expand("{subject}/qc/{subject}.hotspot_coverage.png", subject=PATIENTS)
ALL_QC     += expand("{subject}/annotation/{subject}.Annotations.coding.rare.txt", subject=PATIENTS)
ALL_QC     += expand("{subject}/igv/session_{subject}.xml", subject=PATIENTS)
if len(config['sample_references']) > 0:
    for Tumor in config['sample_references']:
        for Normal in config['sample_references'][Tumor]:
            TumorBam = "{subject}/{sample}/{sample}.bwa.final".format(subject=SAMPLE_TO_SUBJECT[Tumor], sample=Tumor)
            TumorCopy = "{subject}/{sample}/copyNumber/{sample}.count.txt".format(subject=SAMPLE_TO_SUBJECT[Tumor], sample=Tumor)
            NormalBam = "{subject}/{sample}/{sample}.bwa.final".format(subject=SAMPLE_TO_SUBJECT[Normal], sample=Normal)
            NormalCopy = "{subject}/{sample}/copyNumber/{sample}.count.txt".format(subject=SAMPLE_TO_SUBJECT[Normal], sample=Normal)
            pairedCapture[Tumor] = config['target_intervals'][config['sample_captures'][Tumor][0]]
            somaticPairs[Tumor] = [TumorBam + ".bam" , TumorBam + ".bam.bai", NormalBam + ".bam", NormalBam + ".bam.bai"]
            somaticCopy[Tumor] = [NormalCopy, TumorCopy]
###########################################################################
SUBJECT_ANNO = dict([(key, {}) for key in PATIENTS])
def add_to_SUBJECT_ANNO(subject, category, file_list):
    if category not in SUBJECT_ANNO[subject]:
        SUBJECT_ANNO[subject][category] = file_list
    else:
        SUBJECT_ANNO[subject][category].extend(file_list)
###########################################################################
SUBJECT_VCFS = {}
COPY_NUMBER=[]
SOMATIC =[]
for subject in SUBS:
    local  = []
    local.extend([(subject+"/Subject/calls/"+subject+".hapcaller.snpEff.txt"),
              (subject+"/Subject/calls/"+subject+".platypus.snpEff.txt"),
              (subject+"/Subject/calls/"+subject+".bam2mpg.snpEff.txt")])
    if subject not in SUBJECT_VCFS:
        SUBJECT_VCFS[subject] = local
    germline = [w.replace('snpEff','annotated') for w in local]
    add_to_SUBJECT_ANNO(subject,"germline",germline)    

for sample in config['sample_references'].keys():
    local  = []
    subject=SAMPLE_TO_SUBJECT[sample]
    local.extend(
        [ (subject+"/"+sample+"/calls/"+sample+".MuTect.snpEff.txt"),
        (subject+"/"+sample+"/calls/"+sample+".strelka.snvs.snpEff.txt"),
        (subject+"/"+sample+"/calls/"+sample+".strelka.indels.snpEff.txt")
        ]
    )    
    COPY_NUMBER +=[subject+"/"+sample+"/copyNumber/"+sample+".copyNumber.v1.txt"]
    COPY_NUMBER +=[subject+"/"+sample+"/copyNumber/"+sample+".hq.v1.txt"]
    COPY_NUMBER +=[subject+"/"+sample+"/copyNumber/"+sample+".CN.v1.annotated.txt"]
    COPY_NUMBER +=[subject+"/"+sample+"/copyNumber/"+sample+".CN.v1.filtered.txt"]
    COPY_NUMBER +=[subject+"/"+sample+"/copyNumber/"+sample+".copyNumber.v2.txt"]
    COPY_NUMBER +=[subject+"/"+sample+"/copyNumber/"+sample+".hq.v2.txt"]
    COPY_NUMBER +=[subject+"/"+sample+"/copyNumber/"+sample+".CN.v2.annotated.txt"]
    COPY_NUMBER +=[subject+"/"+sample+"/copyNumber/"+sample+".CN.v2.filtered.txt"]
    SOMATIC +=[subject+"/"+sample+"/calls/"+sample+".MuTect.annotated.txt"]
    SOMATIC +=[subject+"/"+sample+"/calls/"+sample+".strelka.snvs.annotated.txt"]
    SOMATIC +=[subject+"/"+sample+"/calls/"+sample+".strelka.indels.annotated.txt"]
    if subject in SUBJECT_VCFS:
        SUBJECT_VCFS[subject].extend(local)

    somatic = [w.replace('snpEff','annotated') for w in local]
    if sample in config['sample_RNASeq']:
        somatic = [w.replace('MuTect.annotated','MuTect.annotated.expressed') for w in somatic]
        somatic = [w.replace('strelka.snvs.annotated','strelka.snvs.annotated.expressed') for w in somatic]
    add_to_SUBJECT_ANNO(subject,"somatic",somatic)
###########################################################################
###########################################################################
ALL_EXPRESSED =[]
expressedPairs = {}
if len(config['sample_RNASeq']) > 0:
    for Tumor in config['sample_RNASeq']:
        for RNASample in config['sample_RNASeq'][Tumor]:
            subject=SAMPLE_TO_SUBJECT[Tumor]
            RNASeqBam    = subject + "/" + RNASample + "/"+RNASample+".star.final.bam"
            expressedPairs[Tumor] = RNASeqBam
            ALL_EXPRESSED += ["{subject}/{sample}/calls/{sample}.MuTect.annotated.expressed.txt".format(subject=SAMPLE_TO_SUBJECT[Tumor],  sample=Tumor)]
            ALL_EXPRESSED += ["{subject}/{sample}/calls/{sample}.strelka.snvs.annotated.expressed.txt".format(subject=SAMPLE_TO_SUBJECT[Tumor], sample=Tumor)]
            
###########################################################################
# we have to do it this way as some samples may not have rna or tumor     #
###########################################################################
varFiles=[]
DBFiles =[]
ActionableFiles =[]
for subject in SUBJECT_ANNO.keys():
    for group in SUBJECT_ANNO[subject].keys():
        DBFiles +=[subject+"/Subject/db/"+subject+"."+group] 
        ActionableFiles +=[subject+ACT_DIR+subject+"."+group+".actionable.txt"] 
        for varFile in SUBJECT_ANNO[subject][group]:
            varFiles.append(varFile)
###########################################################################
localrules: Khanlab_Pipeline, RNASeq, IGV_Session, DBinput, AttachAnnotation, Expressed, vcf2txt, symlink_tophatBam, copyNovoBam, Actionable_Germline, Actionable_RNAseq, Actionable_Somatic, Circos, CoveragePlot, BoxPlot_Hotspot, Sub_HapCall_merge

# ruleorder: fastqMerge > fastqc > BWA > Picard_MarkDup

###########################################################################
#                               RNASeq Rules                              #
###########################################################################
include: NGS_PIPELINE +"/rnaseq_pipeline.rules"
include: NGS_PIPELINE +"/universal.rules"
#include: NGS_PIPELINE +"/scripts/igv_snapshot.rules"
ALL_VCFs =[]
for subject in SUBJECT_VCFS.keys():
    for vcf in SUBJECT_VCFS[subject]:
        vcf = vcf.replace('snpEff.txt', 'raw.vcf')
        ALL_VCFs +=[vcf]
        vcf = vcf.replace('raw.vcf', 'raw.snpEff.vcf')
        ALL_VCFs +=[vcf]

rule Final:
    input: 
        ALL_FASTQ,
        SUB_IGV.values(),
        COPY_NUMBER,
        ALL_VCFs,
        ALL_FASTQ,
        SUB_IGV.values(),
        COPY_NUMBER,
        ALL_VCFs,
        expand("{subject}/annotation/{subject}.Annotations.coding.rare.txt", subject=SUBS),
        expand("{subject}/qc/{subject}.coveragePlot.png", subject=SUBS),
        expand("{subject}/qc/{subject}.circos.png", subject=SUBS),
        expand("{subject}/qc/{subject}.genotyping.txt", subject=SUBS),
        expand("{subject}/qc/{subject}.hotspot_coverage.png", subject=SUBS),
        expand("{subject}/igv/session_{subject}.xml", subject=SUBS),
        "rnaseqDone",
        "QC_AllSamples.txt",
        ALL_QC, ALL_FASTQC,
        ALL_BAMS,
        varFiles,
        DBFiles,
        ActionableFiles
    version: "1.0"
    params: 
        rulename = "Final",
        group    = config["group"],
        wait4job = NGS_PIPELINE + "/scripts/block_for_jobid.pl",
        batch     = config["job_default"]
    shell: """
    #######################
#    find log/ -type f -empty -delete
    find . -group $USER -exec chgrp {params.group} {{}} \;
#    find . \( -type f -user $USER -exec chmod g+r {{}} \; \) , \( -type d -user $USER -exec chmod g+rwxs {{}} \; \)
#    cd /data/khanlab/projects/Genotyping/
#    Final=`sh genotype.sh`
#    perl {params.wait4job} ${{Final}}
#    echo -e "Pipeline finished successfully.\\n\\n\\n\\nRegards,\\nKhanLab\\nOncogenomics Section\\nCCR NCI NIH" |mutt -s "Khanlab NGS Pipeline" `whoami`@mail.nih.gov
    #######################
    """

############
#    Merge fastq files to sample
############
rule fastqMerge:
    input:
        R1=lambda wildcards: [DATA_DIR_fastq + "/"+fq1 for fq1 in config["sample_fastq1"][wildcards.sample] ],
        R2=lambda wildcards: [DATA_DIR_fastq + "/"+fq2 for fq2 in config["sample_fastq2"][wildcards.sample] ]
    output:
        R1=DATA_DIR + "/{sample}_R1.fastq.gz",
        R2=DATA_DIR + "/{sample}_R2.fastq.gz"
    params:
        rulename  = "fastqMerge",
        batch     = config["job_default"]
    run:
        if isinstance(input.R1, list) and len(input.R1) == 1:
            shell("""
                ln -sf  {input.R1} {output.R1};
                ln -sf  {input.R2} {output.R2}
            """)
        else:
            shell("""
                #######################
                cat {input.R1} > {output.R1}
                cat {input.R2} > {output.R2}
                #######################
            """)

############
#    FASTQC
############
rule fastqc:
    input:
        R1=DATA_DIR + "/{sample}_R1.fastq.gz",
        R2=DATA_DIR + "/{sample}_R2.fastq.gz"
    output: 
        "{subject}/{sample}/qc/fastqc/{sample}_R1_fastqc.html", 
        "{subject}/{sample}/qc/fastqc/{sample}_R2_fastqc.html"
    version: config["fastqc"]
    params:
        rulename  = "fastqc",
        batch     = config["job_fastqc"]
    shell: """
    #######################
    module load fastqc/{version}
    fastqc --extract -t ${{SLURM_CPUS_ON_NODE}} -o {wildcards.subject}/{wildcards.sample}/qc/fastqc/ -d /scratch {input[R1]} 
    fastqc --extract -t ${{SLURM_CPUS_ON_NODE}} -o {wildcards.subject}/{wildcards.sample}/qc/fastqc/ -d /scratch {input[R2]} 
    #######################
    """
############
#       BWA
############
rule BWA:
    input: 
        R1=DATA_DIR + "/{sample}_R1.fastq.gz", 
        R2=DATA_DIR + "/{sample}_R2.fastq.gz",
        ref=config["bwaIndex"]
    output: 
        temp("{subject}/{sample}/{sample}.bwa.bam"),
        temp("{subject}/{sample}/{sample}.bwa.bam.bai")
    version: config["bwa"]
    log: "{subject}/{sample}/{sample}.mcf.log"
    params:
        rulename  = "BWA",
        platform  = config["platform"],
        samtools  = config["samtools"],
        ea_utils  = config["ea-utils"],
        adapters  = config["adapters"],
        batch     = config["job_bwa"]
    shell: """
    #######################
    module load bwa/{version}
    module load samtools/{params.samtools}
    module load ea-utils/{params.ea_utils}
    
    R1=/lscratch/${{SLURM_JOBID}}/`basename {input.R1}`
    R2=/lscratch/${{SLURM_JOBID}}/`basename {input.R2}`
    fastq-mcf -C 1000000 -q 2 -p 10 -u -x 20 -o $R1 -o $R2 {params.adapters} <(gunzip -c {input.R1}) <(gunzip -c {input.R2}) > {log} 2>&1

    bwa mem -M \
    -t ${{SLURM_CPUS_ON_NODE}} \
    -R '@RG\tID:{wildcards.sample}\tSM:{wildcards.sample}\tLB:{wildcards.sample}\tPL:{params.platform}' \
    {input.ref} $R1 $R2 | samtools view -Sbh - \
    | samtools sort -m 30000000000 - {wildcards.subject}/{wildcards.sample}/{wildcards.sample}.bwa
    samtools index {wildcards.subject}/{wildcards.sample}/{wildcards.sample}.bwa.bam
    #######################
    """
############
#    Novoalign
############
rule Novoalign:
    input:
        R1=DATA_DIR + "/{sample}_R1.fastq.gz",
        R2=DATA_DIR + "/{sample}_R2.fastq.gz",
        index=config["Novo_index"]
    output:
        temp("{subject}/{sample}/{sample}.novo.bam"),
        temp("{subject}/{sample}/{sample}.novo.bam.bai")
    version: config["novocraft"]
    resources: novoalign=1
    log: "{subject}/{sample}/{sample}.novo.log"
    params:
        rulename  = "Novoalign",
        batch     = config["job_novoalign"],
        ea_utils  = config["ea-utils"],
        adapters  = config["adapters"],
        samtools  = config["samtools"],
        platform  = config["platform"]
    shell: """
    #######################
    module load samtools/{params.samtools}
    module load novocraft/{version}
    module load ea-utils/{params.ea_utils}

    R1=/lscratch/${{SLURM_JOBID}}/`basename {input.R1}`
    R2=/lscratch/${{SLURM_JOBID}}/`basename {input.R2}`
    fastq-mcf -C 1000000 -q 2 -p 10 -u -x 20 -o $R1 -o $R2 {params.adapters} <(gunzip -c {input.R1}) <(gunzip -c {input.R2}) > {log}.fastq-mcf 2>&1
    
    ## novoalignMPI 
    # mpiexec  -envall -host `scontrol show hostname ${{SLURM_NODELIST}} | paste -d',' -s` -np ${{SLURM_NTASKS}} novoalignMPI -c 15 -a -F STDFQ -o SAM \"@RG\\tID:{wildcards.sample}\\tSM:{wildcards.sample}\\tLB:{wildcards.sample}\\tPL:{params.platform}\" -d {input.index} -f $R1 $R2 | samtools view -Sbh - | samtools sort -m 30G - /lscratch/${{SLURM_JOBID}}/out > {log} 2>&1
    
    ## novoalign multitheading
    novoalign -c ${{SLURM_CPUS_ON_NODE}} -F STDFQ -a -o Sync -o SAM "@RG\tID:0A4HLD_Normal-Exon50M\tSM:0A4HLD_Normal-Exon50M\tLB:0A4HLD_Normal-Exon50M\tPL:Illumina" -d {input.index} -f $R1 $R2 | samtools view -Sbh - | samtools sort -m 30G - /lscratch/${{SLURM_JOBID}}/out > {log} 2>&1

    samtools index /lscratch/${{SLURM_JOBID}}/out.bam
    cp -f /lscratch/${{SLURM_JOBID}}/out.bam {wildcards.subject}/{wildcards.sample}/{wildcards.sample}.novo.bam
    cp -f /lscratch/${{SLURM_JOBID}}/out.bam.bai {wildcards.subject}/{wildcards.sample}/{wildcards.sample}.novo.bam.bai
    
    #######################
    """
    
## novoalignMPI - multithreading no tworking
# mpiexec  -envall -host `scontrol show hostname ${{SLURM_NODELIST}} | paste -d',' -s` -np ${{SLURM_NTASKS}} novoalignMPI -c 16 -F STDFQ -o SAM \"@RG\\tID:{wildcards.sample}\\tSM:{wildcards.sample}\\tLB:{wildcards.sample}\\tPL:{params.platform}\" -t 250 --hlimit 7 -p 5,2 -l 30 -e 100 -i 200 100 -a AGATCGGAAGAGCGGTTCAGCAGGAATGCCGAG AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTA -H -d {input.index} -f {input.R1} {input.R2} > /lscratch/${{SLURM_JOBID}}/out.sam

## novoalign - multithreading no tworking
# novoalign -c ${{SLURM_CPUS_ON_NODE}}  -F STDFQ -o SAM \"@RG\\tID:{wildcards.sample}\\tSM:{wildcards.sample}\\tLB:{wildcards.sample}\\tPL:{params.platform}\" -t 250 --hlimit 7 -p 5,2 -l 30 -e 100 -i 200 100 -a AGATCGGAAGAGCGGTTCAGCAGGAATGCCGAG AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTA -H -d {input.index} -f {input.R1} {input.R2} | samtools view -uS - | samtools sort -m 30000000000 - {wildcards.subject}/{wildcards.sample}/{wildcards.sample}.novo

## working example
# novoalign -F STDFQ -d /data/Clinomics/Ref/khanlab/Index/novoalign/chr_all_hg19.nbx -a -f /data/CCRBioinfo/zhujack/projects/TargetOsteosarcoma/NGS/fastq/0A4HLD_Normal-Exon50M_R1.fastq.gz /data/CCRBioinfo/zhujack/projects/TargetOsteosarcoma/NGS/fastq/0A4HLD_Normal-Exon50M_R2.fastq.gz -o SAM "@RG\tID:0A4HLD_Normal-Exon50M\tSM:0A4HLD_Normal-Exon50M\tLB:0A4HLD_Normal-Exon50M\tPL:Illumina" -a AGATCGGAAGAGCGGTTCAGCAGGAATGCCGAG AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTA -o Sync -k -K  /lscratch/${SLURM_JOBID}/out.sam.mismatch -c ${SLURM_CPUS_ON_NODE} | samtools view -uS - | samtools sort -m 30000000000 - 0A4HLD/0A4HLD_Normal-Exon50M/0A4HLD_Normal-Exon50M.novo



############
#       GenotypeFile
############
# Using Older version of samtools for this purpose
rule Genotyping:
    input:
        bam="{base}/{sample}/{sample}.{aligner}.final.bam",
        interval=config["genotypeBed"],
        ref=config["reference"],
        vcf2genotype=NGS_PIPELINE + "/scripts/vcf2genotype.pl",
        vcf2loh=NGS_PIPELINE + "/scripts/vcf2loh.pl",
    output:
        vcf="{base}/{sample}/calls/{sample}.{aligner}.samtools.vcf",
        gt="{base}/{sample}/qc/{sample}.{aligner}.gt",
        loh="{base}/{sample}/qc/{sample}.{aligner}.loh"
    version: config["samtools_old"]
    params:
        rulename  = "Genotyping",
        batch     = config["job_genotype"],
        dest      = config["genotypeDest"]
    shell: """
    #######################
    if [ ! -f {params.dest} ];then
        mkdir -p {params.dest}
    fi
    
    module load samtools/{version}
    samtools mpileup -u -C50 -f {input.ref} -l {input.interval} {input.bam} | bcftools view -gc - >{output.vcf} 
    perl {input.vcf2genotype} {output.vcf} >{output.gt} 
    cp -f {output.gt} {params.dest}/{wildcards.sample}.gt 
    perl {input.vcf2loh} {output.vcf} >{output.loh} 
    #######################
    """
############
# Genotyping On Sample
############
rule SampleGT:
    input:
        gtFiles=lambda wildcards: SUB_GT[wildcards.subject],
        mail =NGS_PIPELINE + "/scripts/tsv2html.sh",
        score=NGS_PIPELINE + "/scripts/scoreGenotyes.pl"
    output:
        "{subject}/qc/{subject}.genotyping.txt",
    version: config["R"]
    params:
        rulename = "SampleGT",
        batch    = config["job_covplot"],
        mail     = config["mail"]
    shell: """
    #######################
    mkdir -p {wildcards.subject}/qc/GT
    mkdir -p {wildcards.subject}/qc/RATIO/
    cp {input.gtFiles} {wildcards.subject}/qc/GT/
    echo Sample >{wildcards.subject}/qc/RATIO/FirstColumn
    
    for file in {wildcards.subject}/qc/GT/*
    do
        sample=`basename ${{file}} .gt`
        echo ${{sample}} >>{wildcards.subject}/qc/RATIO/FirstColumn
        echo ${{sample}} >>{wildcards.subject}/qc/RATIO/${{sample}}.ratio
        for file2 in {wildcards.subject}/qc/GT/*
        do
            perl {input.score} ${{file}} ${{file2}} >>{wildcards.subject}/qc/RATIO/${{sample}}.ratio
        done
    done
    paste {wildcards.subject}/qc/RATIO/FirstColumn {wildcards.subject}/qc/RATIO/*.ratio >{wildcards.subject}/qc/{wildcards.subject}.genotyping.txt
    rm -rf {wildcards.subject}/qc/GT/ {wildcards.subject}/qc/RATIO/
    sed -i 's/Sample_//g' {wildcards.subject}/qc/{wildcards.subject}.genotyping.txt
    sed -i 's/.bwa//g' {wildcards.subject}/qc/{wildcards.subject}.genotyping.txt
    sed -i 's/.star//g' {wildcards.subject}/qc/{wildcards.subject}.genotyping.txt
    sh {input.mail} --name {wildcards.subject} --head {wildcards.subject}/qc/{wildcards.subject}.genotyping.txt | mutt -e "my_hdr Content-Type: text/html" `whoami`@mail.nih.gov {params.mail} -s 'Genotyping Result on {wildcards.subject}'
    #######################
    """
############
#       BamQC
############
rule BamQC:
    input:
        bam="{base}/{sample}/{sample}.bwa.final.bam",
        bai="{base}/{sample}/{sample}.bwa.final.bam.bai",
        interval= lambda wildcards: config['target_intervals'][config['sample_captures'][wildcards.sample][0]].replace('.bed', '.gff') ## it's a list
    output:
        "{base}/{sample}/qc/BamQC/qualimapReport.html"
    version: config["qualimap"]
    params: 
        rulename = "BamQC",
        batch     = config["job_qualimap"],
        outdir     ="{base}/{sample}/qc/BamQC",
    shell: """
    #######################
    MEM=`echo "{params.batch}" |perl -n -e'/--mem=(\d+)/ && print \$1'`
    # module load qualimap/{version}
    ## needs qualimap/2.2-dev, but not on the biowulf
    /data/CCRBioinfo/zhujack/source/qualimap_v2.2/qualimap bamqc -c -bam {input.bam} -outdir {params.outdir} -gff {input.interval} -nt ${{SLURM_CPUS_ON_NODE}} --java-mem-size=${{MEM}}G 
    #######################
    """
############
#       QC_Sum
############
rule QC_Sum:
    input:
        ALL_QC,
        ALL_FASTQC,
        convertor = NGS_PIPELINE + "/scripts/makeQC.pl"
    output:
        "QC_AllSamples.txt"
    version: "v1.1"
    params:
        rulename = "QC_Sum",
        batch    = config['job_default']
    shell: """
    #######################
    perl {input.convertor} `pwd` >{output} 
    #######################
    """
############
#       Samtools flagstat
############
rule flagstat:
    input:    "{base}/{sample}/{sample}.{aligner}.final.bam"
    output: "{base}/{sample}/qc/{sample}.{aligner}.flagstat.txt"
    version: config["samtools"]
    params:
        rulename  = "flagstat",
        batch     = config["job_flagstat"]
    shell: """
    #######################
    module load samtools/{version}
    samtools flagstat {input} > {output}
    #######################
    """
############
#       Reads Count for every Bed Region on bam file
#     capture region corrected
############
rule copyNumber:
    input:
        bam="{base}/{sample}/{sample}.bwa.final.bam",
        interval= lambda wildcards: config['target_intervals'][config['sample_captures'][wildcards.sample][0]],
        flagstat="{base}/{sample}/qc/{sample}.bwa.flagstat.txt",
        tool=NGS_PIPELINE+ "/scripts/copyNumber.sh"
    output:
        "{base}/{sample}/copyNumber/{sample}.count.txt"
    version: config["samtools"]
    params:
        rulename  = "copyNumber",
        batch     = config["job_copynumber"]
    shell: """
    #######################
    module load samtools/{version}
    TotalReads=`samtools view -bh -L {input.interval} {input.bam} |samtools flagstat - |head -1|sed -e 's/\s/\\t/g' |cut -f 1`
    #TotalReads=`head -1 {input.flagstat} | sed -e 's/\s/\\t/g' |cut -f 1`
    split -d -l 12000 {input.interval} /lscratch/${{SLURM_JOBID}}/input
    for file in /lscratch/${{SLURM_JOBID}}/input*
    do
        sh {input.tool} ${{TotalReads}} ${{file}} {input.bam} ${{file}}.out &
    done
    wait;
    cat /lscratch/${{SLURM_JOBID}}/input*.out >{output}
    #######################
    """
############
#       Somatic Copy Number
############
rule CN_LRR:
    input:
        files=lambda wildcards: somaticCopy[wildcards.Tumor],
        ref=config["gene_coord"],
        index=config["reference"].replace('.fasta', '.index.txt'),
        tool=NGS_PIPELINE+ "/scripts/AddGene.pl",
        cgc    = config["annovar_data"]+"geneLists/combinedList_030816",
        filter=NGS_PIPELINE+ "/scripts/filterCNV.pl"
    output:
        out="{subject}/{Tumor}/copyNumber/{Tumor}.copyNumber.v1.txt",
        hq="{subject}/{Tumor}/copyNumber/{Tumor}.hq.v1.txt",
        final="{subject}/{Tumor}/copyNumber/{Tumor}.CN.v1.annotated.txt",
        filtered="{subject}/{Tumor}/copyNumber/{Tumor}.CN.v1.filtered.txt"    
    params:
        rulename = "CN_LRR",
        batch    = config["job_lrr"],
        tool     = NGS_PIPELINE+ "/scripts/ListStatistics.R"
    shell: """
    #######################
    module load R
    module load bedtools/2.25.0
    mkdir -p {wildcards.subject}/Actionable/
    echo -e "#Chr\\tStart\\tEnd\\tNormalCoverage\\tTumorCoverage\\tRatio\\tLRR\\tGene(s)\\tStrand(s)" >{output.out}    
    paste {input.files} |cut -f 1-4,8 |awk '{{OFS="\\t"}};{{print $1,$2,$3,$4,$5,($5+1)/($4+1),log(($5+1)/($4+1))/log(2)}}' >{output.out}.temp1
    
    intersectBed -a {input.ref} -b {input.files[0]} >{output.out}.temp
    perl {input.tool} {output.out}.temp {output.out}.temp1 >>{output.out}

    awk '{{if($4>=30) print $0}}' {output.out} >{output.hq}
    median=`cut -f7 {output.hq}|grep -v LRR | {params.tool} --stat median --file - | grep -v WARNING`
    MAD=`cut -f7 {output.hq}|grep -v LRR | {params.tool} --stat MAD --file - | grep -v WARNING`
    min=`echo "${{median}}-(2*${{MAD}})"|bc`
    max=`echo "${{median}}+(2*${{MAD}})"|bc`

    perl {input.filter} filter {output.out} ${{min}} ${{max}} {input.cgc} |sortBed -faidx {input.index} -header -i - >{output.final}
    cp -f {output.final} {wildcards.subject}{ACT_DIR}/{wildcards.Tumor}.copyNumber.v1.txt
    geneList=`grep -P "Gain|Loss" {output.final} |cut -f 11 |sort |uniq |grep -v "^-$"`
    
    head -1 {output.final} >{output.filtered}
    for gene in ${{geneList}};
    do
        awk -v gene=${{gene}} '{{if($11 == gene) print $0}}' {output.final}
    done |sort |uniq |sortBed -faidx {input.index} -header -i - >>{output.filtered}
    cp -f {output.filtered} {wildcards.subject}{ACT_DIR}/{wildcards.Tumor}.CN.v1.filtered.txt
    rm -rf {output.out}.temp1 {output.out}.temp

    #######################
    """
############
#       Somatic Copy Number LRR Corrected
############
rule CN_LRR1:
    input:
        files=lambda wildcards: somaticCopy[wildcards.Tumor],
        ref=config["gene_coord"],
        index=config["reference"].replace('.fasta', '.index.txt'),
        tool=NGS_PIPELINE+ "/scripts//AddGene.pl",
        cgc    = config["annovar_data"]+"geneLists/combinedList_030816",
        filter=NGS_PIPELINE+ "/scripts/filterCNV.pl"
    output:
        out="{subject}/{Tumor}/copyNumber/{Tumor}.copyNumber.v2.txt",
        hq="{subject}/{Tumor}/copyNumber/{Tumor}.hq.v2.txt",
        final="{subject}/{Tumor}/copyNumber/{Tumor}.CN.v2.annotated.txt",
        filtered="{subject}/{Tumor}/copyNumber/{Tumor}.CN.v2.filtered.txt"
    params:
        rulename = "CN_LRR1",
        batch    = config["job_lrr"],
        tool     = NGS_PIPELINE+ "/scripts/ListStatistics.R"
    shell: """
    #######################
    module load R
    module load bedtools/2.25.0
    mkdir -p {wildcards.subject}/Actionable/
    
    echo -e "#Chr\\tStart\\tEnd\\tNormalCoverage\\tTumorCoverage\\tRatio\\tLRR\\tGene(s)\\tStrand(s)" >{output.out}
    paste {input.files} |cut -f 1-4,8 |awk '{{OFS="\\t"}};{{print $1,$2,$3,$4,$5,($5+1)/($4+1),log(($5+1)/($4+1))/log(2)}}' >{output.out}.temp1
    
    intersectBed -a {input.ref} -b {input.files[0]} >{output.out}.temp
    perl {input.tool} {output.out}.temp {output.out}.temp1 >>{output.out}

    # Created the first file

    awk '{{if($4>=30) print $0}}' {output.out} >{output.hq}
    median=`cut -f7 {output.hq}|grep -v LRR | {params.tool} --stat median --file - | grep -v WARNING`
    
    corr_factor=`echo "0 - ${{median}}"|bc`
    echo -e "#Chr\\tStart\\tEnd\\tNormalCoverage\\tTumorCoverage\\tRatio\\tLRR\\tGene(s)\\tStrand(s)" >{output.out}.corrected
    grep -v Ratio {output.out} |awk -v factor=${{corr_factor}} '{{OFS="\\t"}};{{print $1,$2,$3,$4,$5,$6,$7 + (factor),$8,$9}}' >>{output.out}.corrected

    # Created corrected file
    awk '{{if($4>=30) print $0}}' {output.out}.corrected >{output.hq}

    median=`cut -f7 {output.hq}| grep -v LRR |sort -n |{params.tool} --stat median --file - | grep -v WARNING`
    
    MAD=`cut -f7 {output.hq}|grep -v LRR |{params.tool} --stat MAD --file - | grep -v WARNING`
    min=`echo "${{median}}-(2.5*${{MAD}})"|bc`
    max=`echo "${{median}}+(2.5*${{MAD}})"|bc`

    perl {input.filter} filter {output.out}.corrected ${{min}} ${{max}} {input.cgc} |sortBed -faidx {input.index} -header -i - >{output.final}
    cp -f {output.final} {wildcards.subject}{ACT_DIR}/{wildcards.Tumor}.copyNumber.v2.txt
    geneList=`grep -P "Gain|Loss" {output.final} |cut -f 11 |sort |uniq |grep -v "^-$"`

    head -1 {output.final} >{output.filtered}
    for gene in ${{geneList}};
    do
        awk -v gene=${{gene}} '{{if($11 == gene) print $0}}' {output.final}
    done |sort |uniq |sortBed -faidx {input.index} -header -i - >>{output.filtered}
    cp -f {output.filtered} {wildcards.subject}{ACT_DIR}/{wildcards.Tumor}.CN.v2.filtered.txt
    rm -rf {output.out}.temp1 {output.out}.temp
    #######################
    """
############
#       Hotspot Coverage
############
rule HotSpotCoverage:
    input:
        bam="{base}/{sample}/{sample}.{aligner}.final.bam",
        interval=config["hotspot_intervals"]
    output: "{base}/{sample}/qc/{sample}.{aligner}.hotspot.depth"
    version: config["bedtools"]
    params:
        rulename  = "HotSpotCoverage",
        batch     = config["job_hotspot"],
        samtools  = config["samtools"]
    shell: """
    #######################
    module load samtools/{params.samtools} 
    module load bedtools/{version}
    samtools view -hF 0x400 -q 30 {input.bam} | samtools view -ShF 0x4 - | samtools view -SuF 0x200 - | bedtools coverage -abam - -b {input.interval} >{output}
    #######################
    """
############
# Coverage 
############
rule Coverage:
    input:
        bam="{subject}/{sample}/{sample}.bwa.final.bam",
        bai="{subject}/{sample}/{sample}.bwa.final.bam.bai",
        interval= lambda wildcards: config['target_intervals'][config['sample_captures'][wildcards.sample][0]]
    output:
        "{subject}/{sample}/qc/{sample}.bwa.coverage.txt"
    version: config["bedtools"]
    params:
        rulename = "Coverage",
        batch    = config["job_bedtools"]
    shell: """
    #######################
    module load bedtools/{version}
    bedtools coverage -abam {input.bam} -b {input.interval} -hist |grep "^all" > {output}
    #######################
    """
############
# CoveragePlot
############
rule CoveragePlot:
    input: 
        covFiles=lambda wildcards: SUB_COV[wildcards.subject],
        coverage =NGS_PIPELINE + "/scripts/coverage.R"
    output: "{subject}/qc/{subject}.coveragePlot.png",
    version: config["R"]
    params:
        rulename = "CoveragePlot",
        batch    = config["job_covplot"]
    shell: """
    #######################
        
    cp -f {input.covFiles} {wildcards.subject}/qc/ 
        
    module load R/{version}
    R --vanilla --slave --silent --args {wildcards.subject}/qc/ {output} {wildcards.subject} <{input.coverage}
    #######################
    """
rule IGV_Session:
    input: bams=lambda wildcards: SUB_IGV[wildcards.subject]
    output: "{subject}/igv/session_{subject}.xml"
    message: "Making IGV session xml file for {wildcards.subject}"
    params:
        rulename = "IGV_Session",
        work_dir =  WORK_DIR
    shell: """
    #######################
    dir=`echo {params.work_dir} | sed -e 's/\/data\/khanlab/K:/g'`
    echo "<?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?>" >{output}
    echo "<Global genome=\\"hg19\\" locus=\\"\\" version=\\"3\\">" >>{output}
    echo "\t<Resources>" >>{output}
    for BAM in {input.bams}
    do    
        bam=`echo "${{dir}}/${{BAM}}" |sed -e 's/\//\\\\\\/g'`
        echo "\t\t<Resource path=\\"${{bam}}\\"/>" >>{output}
    done 
    echo "\t</Resources>" >>{output}
    echo "</Global>" >>{output}
    #######################
    """
############
# Circos Plot
############
rule Circos:
    input:
        lohFiles=lambda wildcards: SUB_LOH[wildcards.subject],
        circos =NGS_PIPELINE + "/scripts/circos.R"
    output:
        "{subject}/qc/{subject}.circos.png",
    version: config["R"]
    params:
        rulename = "Circos",
        batch    = config["job_covplot"]
    shell: """
    #######################
    cp -f {input.lohFiles} {wildcards.subject}/qc/
    module load R/{version}
    R --vanilla --slave --silent --args {wildcards.subject}/qc/ {output} {wildcards.subject} <{input.circos}
    #######################
    """
############
# Box Plot Hotspot
############
rule BoxPlot_Hotspot:
    input:
        covFiles=lambda wildcards: SUB_HOT[wildcards.subject],
        boxplot =NGS_PIPELINE + "/scripts/boxplot.R"
    output:
        "{subject}/qc/{subject}.hotspot_coverage.png",
    version: config["R"]
    params:
        rulename = "BoxPlot_Hotspot",
        batch    = config["job_covplot"]
    shell: """
    #######################
    cp -f {input.covFiles} {wildcards.subject}/qc/
    module load R/{version}
    R --vanilla --slave --silent --args {wildcards.subject}/qc/ {output} {wildcards.subject} <{input.boxplot}
    #######################
    """
############
#       Picard Mark Duplicates
############
rule Picard_MarkDup:
    input:
        bam="{subject}/{sample}/{sample}.{base}.bam",
        bai="{subject}/{sample}/{sample}.{base}.bam.bai"
        
    output: 
        bam=temp("{subject}/{sample}/{sample}.{base}.dd.bam"),
        index=temp("{subject}/{sample}/{sample}.{base}.dd.bam.bai"),
        metrics="{subject}/{sample}/qc/{base}.markdup.txt"
    version: config["picard"]
    params:
        rulename  = "Picard_MarkDup",
        batch     = config["job_markdup"],
        samtools  = config["samtools"]    
    shell: """
    #######################
    MEM=`echo "{params.batch}" |perl -n -e'/--mem=(\d+)/ && print \$1'`
    module load picard/{version}
    java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $PICARDJARPATH/MarkDuplicates.jar AS=true M={output.metrics} I={input.bam} O={output.bam} REMOVE_DUPLICATES=false VALIDATION_STRINGENCY=SILENT
    module load samtools/{params.samtools}
    samtools index {output.bam}
    ######################
    """
############
# copy novo bam to novo.final bam
############
rule copyNovoBam:
    input:
        bam="{base}/{sample}/{sample}.novo.dd.bam",
        bai="{base}/{sample}/{sample}.novo.dd.bam.bai",
    output:
        bam="{base}/{sample}/{sample}.novo.final.bam",
        bai="{base}/{sample}/{sample}.novo.final.bam.bai",
    params:
        rulename = "copyNovoBam",
        batch    = config['job_default']
    shell: """
    #######################
    
    mv {input.bam} {output.bam}
    mv {input.bai} {output.bai}
    
    #######################
    """
############
#       GATK Best Practices
############
rule GATK:
    input:     bam="{base}/{sample}/{sample}.bwa.dd.bam",
        bai="{base}/{sample}/{sample}.bwa.dd.bam.bai",
        ref=config["reference"],
        phase1=config["1000G_phase1"],
        mills=config["Mills_and_1000G"]
    output:
        bam="{base}/{sample}/{sample}.bwa.final.bam",
        index="{base}/{sample}/{sample}.bwa.final.bam.bai",
    version: config["GATK"]
    params:
        rulename  = "GATK",
        batch     = config["job_gatk"]
    shell: """
    #######################
    MEM=`echo "{params.batch}" |perl -n -e'/--mem=(\d+)/ && print \$1'`
    module load GATK/{version}
    java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $GATK_JAR -T RealignerTargetCreator -nt 24 -R {input.ref} -known {input.phase1} -known {input.mills} -I {input.bam} -o /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.realignment.intervals 
    java -Xmx4g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $GATK_JAR -T IndelRealigner -R {input.ref} -known {input.phase1} -known {input.mills} -I {input.bam} --targetIntervals /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.realignment.intervals -o /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.lr.bam 
    java -Xmx4g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $GATK_JAR -T BaseRecalibrator -nct 8 -R {input.ref} -knownSites {input.phase1} -knownSites {input.mills} -I /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.lr.bam -o /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.recalibration.matrix.txt
    java -Xmx4g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $GATK_JAR -T PrintReads -nct 8 -R {input.ref} -I /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.lr.bam -o {output.bam} -BQSR /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.recalibration.matrix.txt 
    mv -f {wildcards.base}/{wildcards.sample}/{wildcards.sample}.bwa.final.bai {output.index}
    ######################
    """
############
#    Bam2MPG
############
rule Bam2MPG:
    input:
        bam="{subject}/{sample}/{sample}.novo.final.bam",
        bai="{subject}/{sample}/{sample}.novo.final.bam.bai",
        ref=config["reference"],
        interval= lambda wildcards: config['target_intervals'][config['sample_captures'][wildcards.sample][0]]
    output:
        snps="{subject}/{sample}/calls/{sample}.bam2mpg.vcf.gz",
    version: config["bam2mpg"]
    params:
        rulename  = "Bam2MPG",
        batch     = config["job_bam2mpg"],
        samtools  = config["samtools"],
        vcftools  = config["vcftools"]
    shell: """
    #######################
    if [ -f {wildcards.subject}/{wildcards.sample}/calls/{wildcards.sample}.bam2mpg.vcf.gz ]; then
            rm -rf {wildcards.subject}/{wildcards.sample}/calls/{wildcards.sample}.bam2mpg.vcf.*
        
    fi

    module load bam2mpg/{version}
    module load vcftools/{params.vcftools}
    for CHR in `seq 1 22` X Y;
    do
    bam2mpg --qual_filter 20 -bam_filter '-q31' --region chr${{CHR}} --only_nonref --snv_vcf /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.vcf --div_vcf /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.vcf {input.ref} {input.bam} &
    done
    wait

    for CHR in `seq 1 22` X Y
    do
        cat /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.vcf | vcf-sort >/lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.tmp.vcf
        mv -f /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.tmp.vcf /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.vcf
        bgzip /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.vcf
        cat /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.vcf | vcf-sort >/lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.tmp.vcf
        mv -f /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.tmp.vcf /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.vcf
        bgzip /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.vcf
        tabix -p vcf /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.vcf.gz
        tabix -p vcf /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.vcf.gz
    done
    echo "Combine chr level vcf files"
    vcf-concat /lscratch/${{SLURM_JOBID}}/chr*{wildcards.sample}.*.vcf.gz >/lscratch/${{SLURM_JOBID}}/{wildcards.sample}.snps.vcf
    echo "Restrict to Bed file"

    vcftools --vcf /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.snps.vcf --bed {input.interval} --out {wildcards.sample} --recode --keep-INFO-all

    sed -e 's/SAMPLE/{wildcards.sample}/g' {wildcards.sample}.recode.vcf |vcf-sort >{wildcards.subject}/{wildcards.sample}/calls/{wildcards.sample}.bam2mpg.vcf

    bgzip {wildcards.subject}/{wildcards.sample}/calls/{wildcards.sample}.bam2mpg.vcf
    tabix -f -p vcf {wildcards.subject}/{wildcards.sample}/calls/{wildcards.sample}.bam2mpg.vcf.gz
    rm -rf {wildcards.sample}.recode.vcf
    
    #######################
    """
############
# Subject Bam2MPG
############
rule Sub_MPG:
    input:
        vcf=lambda wildcards: SUB_MPG[wildcards.subject],
    output:
        vcf="{subject}/Subject/calls/{subject}.bam2mpg.raw.vcf"
    version: config["vcftools"]
    params:
        rulename = "Sub_MPG",
        batch    = config["job_default"],
        vcforder = NGS_PIPELINE + "/scripts/vcfOrderCol.R"
    shell: """
    #######################
    module load vcftools/{version}
    module load R
    vcf-merge {input.vcf} > {output.vcf}.tmp
    {params.vcforder} -i {output.vcf}.tmp -o {output.vcf}
    rm -rf {output.vcf}.tmp
    #######################
    """
############
#       MuTect
############
rule MuTect:
    input:
        lambda wildcards: somaticPairs[wildcards.Tumor],
        ref=config["reference"],
        dbsnp=config["dbsnp"],
        cosmic=config["cosmic"],
        interval=lambda wildcards: pairedCapture[wildcards.Tumor]
    output:
        vcf="{subject}/{Tumor}/calls/{Tumor}.MuTect.raw.vcf",
        call_stats="{subject}/{Tumor}/qc/{Tumor}.mutect.call_stats.txt",
        coverage="{subject}/{Tumor}/qc/{Tumor}.mutect.coverage.wig.txt"
    version: config["MuTect"]
    params:
        rulename = "MuTect",
        batch    = config["job_mutect"],
        vcforder = NGS_PIPELINE + "/scripts/vcfOrderCol.R",
        mt       = "--max_alt_allele_in_normal_fraction 0.05 --max_alt_alleles_in_normal_count 4"
    shell: """
    #######################
    MEM=`echo "{params.batch}" |perl -n -e'/--mem=(\d+)/ && print \$1'`
    gawk '{{print $1 "\t" $2-10 "\t" $3+10}}' {input.interval} > /lscratch/${{SLURM_JOBID}}/target_intervals.bed
    module load muTect/{version}
    module load R
    java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $MUTECT_JAR -T MuTect \
        --reference_sequence {input.ref} \
        --cosmic {input.cosmic} \
        --dbsnp {input.dbsnp} \
        --input_file:normal {input[2]} \
        --input_file:tumor {input[0]} \
        --intervals  /lscratch/${{SLURM_JOBID}}/target_intervals.bed \
        --coverage_file {output.coverage} \
        --out {output.call_stats} \
        --vcf {output.vcf}.vcf \
        {params.mt}
    
    {params.vcforder} -i {output.vcf}.vcf -o {output.vcf}
    rm -rf {output.vcf}.vcf
    #######################
    """
############
#       Strelka
############
rule Strelka:
    input:
        lambda wildcards: somaticPairs[wildcards.Tumor],
        ref=config["reference"],
        config=config["strelka_config"],
        interval=lambda wildcards: pairedCapture[wildcards.Tumor]
    output:
        snps="{subject}/{Tumor}/calls/{Tumor}.strelka.snvs.raw.vcf",
        indels="{subject}/{Tumor}/calls/{Tumor}.strelka.indels.raw.vcf"
    version: config["strelka"]
    params:
        rulename = "Strelka",
        batch    = config["job_strelka"],
        vcftools = config["vcftools"]
    shell: """
    #######################
    gawk '{{print $1 "\t" $2-10 "\t" $3+10}}' {input.interval} > /lscratch/${{SLURM_JOBID}}/target_intervals.bed
    module load strelka/{version}
    configureStrelkaWorkflow.pl --normal={input[2]} --tumor={input[0]}\
    --ref={input.ref} --config={input.config} --output-dir=/lscratch/${{SLURM_JOBID}}/strelka 
    make -j ${{SLURM_CPUS_PER_TASK}} -f /lscratch/${{SLURM_JOBID}}/strelka/Makefile 
    module load vcftools/{params.vcftools}
    vcftools --vcf /lscratch/${{SLURM_JOBID}}/strelka/results/passed.somatic.snvs.vcf --bed /lscratch/${{SLURM_JOBID}}/target_intervals.bed --out {output.snps} --recode --keep-INFO-all
    mv -f {output.snps}.recode.vcf {output.snps}
    vcftools --vcf /lscratch/${{SLURM_JOBID}}/strelka/results/passed.somatic.indels.vcf --bed /lscratch/${{SLURM_JOBID}}/target_intervals.bed --out {output.indels} --recode --keep-INFO-all
    mv -f {output.indels}.recode.vcf {output.indels}
    NORMAL=`basename {input[2]} .bwa.final.bam`
    sed -i "s/FORMAT\\tNORMAL\\tTUMOR/FORMAT\\t${{NORMAL}}\\t{wildcards.Tumor}/g" {output.snps}
    sed -i "s/FORMAT\\tNORMAL\\tTUMOR/FORMAT\\t${{NORMAL}}\\t{wildcards.Tumor}/g" {output.indels}
    
    #######################
    """
############
# Subject Hapcaller
############
rule Sub_HapCall:
    input:
        bams=lambda wildcards: SUB_BAMS[wildcards.subject],
        ref=config["reference"],
        dbsnp=config["dbsnp"],
        interval= lambda wildcards: config['target_intervals'][config['subject_captures'][wildcards.subject][0]]
    output:
        vcf=temp("{subject}/Subject/calls/{subject}.hapcaller.{chr}.raw.vcf")
    version: config["GATK"]
    params:
        rulename = "Sub_HapCall",
        batch    = config["job_hapcaller"]
    run:
        bamFiles = " ".join('-I ' + bam for bam in input.bams)
        shell("""
    #######################
    MEM=`echo "{params.batch}" |perl -n -e'/--mem=(\d+)/ && print \$1'`
    module load GATK/{version}
    gawk '{{print $1 "\t" $2-1 "\t" $3}}' {input.interval} | grep -w {wildcards.chr} > /lscratch/${{SLURM_JOBID}}/target_intervals.bed
    java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $GATK_JAR -T HaplotypeCaller -R {input.ref} {bamFiles} -L /lscratch/${{SLURM_JOBID}}/target_intervals.bed -o {output.vcf} --dbsnp {input.dbsnp} -mbq 20 -mmq 30
    #######################
    """)

rule Sub_HapCall_merge:
    input: ["{subject}/Subject/calls/{subject}.hapcaller." + chr + ".raw.vcf" for chr in chrList]
    output: "{subject}/Subject/calls/{subject}.hapcaller.raw.vcf"
    version: config["vcftools"]
    params:
        rulename = "Sub_HapCall_merge",
        batch    = config["job_default"]
    shell: """
    module load vcftools/{version}
    vcf-concat {input} | vcf-sort > {output}
"""
    
############
# Subject Platypus
############
rule Sub_Platypus:
    input:
        bams=lambda wildcards: SUB_BAMS[wildcards.subject],
        ref=config["reference"],
        dbsnp=config["dbsnp"],
        interval= lambda wildcards: config['target_intervals'][config['subject_captures'][wildcards.subject][0]]##changed from sample_captures
    output:
        vcf="{subject}/Subject/calls/{subject}.platypus.raw.vcf"
    version: config["platypus"]
    log: "log/platypus.{subject}"
    params:
        rulename = "Sub_Platypus",
        batch    = config["job_platypus"]
    shell: """
    #######################
    module load platypus/{version}
    LIST=`echo {input.bams}|sed -e 's/ /,/g'`
    platypus callVariants --nCPU=${{SLURM_CPUS_ON_NODE}} --bufferSize=1000000 --maxReads=100000000 --bamFiles=${{LIST}} --regions={input.interval} --output={output.vcf} --refFile={input.ref}  --logFileName={log}
    sed -i 's/.bwa.final//g' {output.vcf}
    #######################
    """
############
#       FreeBayes     ** Not in use **
############
rule  FreeBayes:
    input:
        bam="{subject}/{sample}/{sample}.bwa.final.bam",
        bai="{subject}/{sample}/{sample}.bwa.final.bam.bai",
        ref=config["reference"],
        interval= lambda wildcards: config['target_intervals'][config['sample_captures'][wildcards.sample][0]]
    output:
        vcf="{subject}/{sample}/calls/{sample}.freebayes.vcf"
    version: config["freebayes"]
    params:
        rulename = "FreeBayes",
        batch    = config["job_freebayes"],
        vcftools = config["vcftools"]
    shell: """
    #######################
    module load freebayes/{version}
    freebayes -f {input.ref} --haplotype-length 50 -b {input.bam} -v {output.vcf}
    module load vcftools/{params.vcftools}
    vcftools --vcf {output} --bed {input.interval} --out {output.vcf} --recode --keep-INFO-all
    mv -f {output.vcf}.recode.vcf {output.vcf}
    #######################
    """
############
#    snpEff
############
rule snpEff:
    input:
        vcf="{subject}/calls/{base}.raw.vcf",
        ref=config["reference"],
        snpEff_config=config["snpEff_config"],
    output:
        eff="{subject}/calls/{base}.raw.snpEff.vcf"
    version: config["snpEff"]
    params:
        rulename      ="snpEff",
        batch          =config["job_snpeff"],
        snpEff_genome =config["snpEff_genome"],
        annovar       =config["annovar"]
    shell: """
    #######################
    MEM=`echo "{params.batch}" |perl -n -e'/--mem=(\d+)/ && print \$1'`
    module load snpEff/{version}
    java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar ${{SNPEFFHOME}}/SnpSift.jar dbnsfp -c {input.snpEff_config} -a {input.vcf} | java -Xmx${{MEM}}g -jar ${{SNPEFFHOME}}/snpEff.jar -t -canon {params.snpEff_genome} > {output.eff}
    #######################
    """
############
#       vcf2txt
############
rule vcf2txt:
    input:
        eff="{subject}/calls/{base}.raw.snpEff.vcf",
        vcf2txt=NGS_PIPELINE + "/scripts/vcf2txt.pl"
    output:
        txt="{subject}/calls/{base}.snpEff.txt"
    params:
        rulename      ="vcf2txt",
        batch         =config["job_default"],
        annovar       =config["annovar"]
    shell: """
    #######################
    module load annovar/{params.annovar}
    perl {input.vcf2txt} {input.eff} /spin1/scratch/ >{output.txt}
    #######################
    """

############
#    MakeList
############
rule FormatInput:
    input:
        txtFiles=lambda wildcards: SUBJECT_VCFS[wildcards.subject],
        convertor= NGS_PIPELINE + "/scripts/MakeAnnotationInputs.pl"
    output: 
        temp("{subject}/annotation/AnnotationInput.anno"), 
        temp("{subject}/annotation/AnnotationInput.sift")
    version: config["annovar"]
    params:
        rulename   = "FormatInput",
        batch      = config["job_default"],
        fAEV       = NGS_PIPELINE + "/scripts/findAlreadyExistingVariants.pl"
    shell: """
    #######################
    module load annovar/{version}
    cut -f 1-5 {input.txtFiles} |sort |uniq > {wildcards.subject}/annotation/allSites
    perl {params.fAEV} {wildcards.subject}/annotation/allSites annovar/AnnotationInput.final.txt {wildcards.subject}/annotation/AnnotationInput.annotations.final.txt {wildcards.subject}/annotation/AnnotationInput
    perl {input.convertor} {wildcards.subject}/annotation/AnnotationInput
    rm -rf "{wildcards.subject}/annotation/AnnotationInput.pph",
    #######################
    """
############
#    Custom Annotation
############
rule Annotation:
    input: 
        "{subject}/annotation/AnnotationInput.anno",
        TableAnnovar=NGS_PIPELINE + "/scripts/TableAnno.sh",
        custom     =NGS_PIPELINE + "/scripts/addAnnotation.pl"
    output:
        temp("{subject}/annotation/AnnotationInput.docm")
    version: config["annovar"]
    params:
        rulename   = "Annotation",
        batch      = config["job_annovar"],
        RefData    = config["annovar_data"],
        build      = config["build"],
    shell: """
    #######################
    module load annovar/{version}
    sh {input.TableAnnovar} {wildcards.subject}/annotation AnnotationInput {input.custom} {params.RefData}
    #######################
    """
############
#       SIFT
############
rule SIFT:
    input:
        sift="{subject}/annotation/AnnotationInput.sift",
        convertor  = NGS_PIPELINE + "/scripts/ParseSIFT.pl"
    output: 
        temp("{subject}/annotation/AnnotationInput.sift.out")
    version: config["SIFT"]
    params:
        rulename   = "SIFT",
        batch      = config["job_SIFT"],
        build      = config["SIFTbuild"]
    shell: """
    #######################
    if [ -s {input.sift} ]; then
        module load python/2.7.4
        module load SIFT/{version}
        DIR=`pwd`
        cd ${{DIR}}/`dirname {input.sift}`
        FILE=`basename {input.sift}` 
        SIFT_exome_nssnvs.pl -i ${{FILE}} -d $SIFTDB/Human_db_37 -o $SIFT_SCRATCHDIR -z ${{DIR}}/`dirname {input.sift}`/${{FILE}}.sift_predictions.tsv
        perl {input.convertor} ${{DIR}}/`dirname {input.sift}`/${{FILE}}.sift_predictions.tsv >${{DIR}}/`dirname {input.sift}`/${{FILE}}.out
    else
        echo -e "Chr\\tStart\\tEnd\\tRef\\tAlt\\tSIFT Prediction\\tSIFT Score" >{output}
    fi
    #######################
    """
############
#       PPH2   ** Not in use**
############
rule PPH2:
    input:
        pph="{subject}/annotation/AnnotationInput.pph",
        convertor  = NGS_PIPELINE + "/scripts/ParsePPH2.pl"
    output: "{subject}/annotation/AnnotationInput.pph2.out"
    version: config["polyphen2"]
    params:
        rulename   = "PPH2",
        batch      = config["job_PPH2"],
    shell: """
    #######################
    module load polyphen2/{version}
    if [ -s {input.pph} ]; then
        mapsnps.pl -c -g hg19 -U -y {input.pph}.intermediate {input.pph}
        pph_swarm.pl {input.pph}.intermediate -d /scratch/`whoami`/${{RANDOM}}${{RANDOM}} -o {wildcards.subject}/annotation/AnnotationInput.pph2.intermediate.txt --partition ${{SLURM_JOB_PARTITION}} --block
        perl {input.convertor}  {wildcards.subject}/annotation/AnnotationInput.pph2.intermediate.txt >{output}
    else 
        touch {output}
    fi
    rm -rf {wildcards.subject}/annotation/AnnotationInput.pph.inter*
    #######################
    """
############
#    Combine Annotation
############
rule CombineAnnotation:
    input:
        anno="{subject}/annotation/AnnotationInput.docm", 
        sift="{subject}/annotation/AnnotationInput.sift.out", 
        convertor  = NGS_PIPELINE + "/scripts/CombineAnnotations.pl",
        geneanno   = NGS_PIPELINE + "/scripts/GeneAnnotation.pl",
        filter     = NGS_PIPELINE + "/scripts/filterVariants.pl",
        coding     = NGS_PIPELINE + "/scripts/ProteinCoding.pl",
        blacklisted      = config["annovar_data"]+ "hg19_blacklistedSites.txt"
    output: "{subject}/annotation/{subject}.Annotations.coding.rare.txt"
    version: "1.0"
    params:
        rulename   = "CombineAnnotation",
        batch       = config["job_Combine"],
        dataDir    = config["annovar_data"]
    shell: """
    #######################
    echo "{wildcards.subject}/annotation/AnnotationInput
{wildcards.subject}/annotation/AnnotationInput.anno.gene
{wildcards.subject}/annotation/AnnotationInput.anno.exac.3
{wildcards.subject}/annotation/AnnotationInput.anno.clinseq
{wildcards.subject}/annotation/AnnotationInput.anno.cadd
{wildcards.subject}/annotation/AnnotationInput.sift.out
{wildcards.subject}/annotation/AnnotationInput.clinvar
{wildcards.subject}/annotation/AnnotationInput.anno.cosmic
{wildcards.subject}/annotation/AnnotationInput.hgmd
{wildcards.subject}/annotation/AnnotationInput.match
{wildcards.subject}/annotation/AnnotationInput.docm
{wildcards.subject}/annotation/AnnotationInput.candl
{wildcards.subject}/annotation/AnnotationInput.tcc
{wildcards.subject}/annotation/AnnotationInput.mcg
{wildcards.subject}/annotation/AnnotationInput.civic
{wildcards.subject}/annotation/AnnotationInput.anno.pcg" >{wildcards.subject}/annotation/list
    perl {input.convertor} {wildcards.subject}/annotation/list >{output}
    perl {input.geneanno} {params.dataDir}/hg19_ACMG.txt {output} >>{wildcards.subject}/annotation/AnnotationInput.annotations.final.txt
    perl {input.coding} {wildcards.subject}/annotation/AnnotationInput.annotations.final.txt | perl {input.filter} - {input.blacklisted} 0.1 |sort -n |uniq >{output}

    rm -rf {wildcards.subject}/annotation/AnnotationInput.pph {wildcards.subject}/annotation/AnnotationInput.anno.* {wildcards.subject}/annotation/AnnotationInput.hgmd {wildcards.subject}/annotation/AnnotationInput.match {wildcards.subject}/annotation/AnnotationInput.candl {wildcards.subject}/annotation/AnnotationInput.tcc {wildcards.subject}/annotation/AnnotationInput.mcg {wildcards.subject}/annotation/AnnotationInput.civic {wildcards.subject}/annotation/AnnotationInput.anno.pcg {wildcards.subject}/annotation/AnnotationInput.clinvar {wildcards.subject}/annotation/AnnotationInput {wildcards.subject}/annotation/allSites  
    #######################
    """
############
#    Add Annotation back to sample level file 
############
rule AttachAnnotation:
    input:
        txt="{subject}/{base1}/calls/{base}.snpEff.txt",
        ref="{subject}/annotation/{subject}.Annotations.coding.rare.txt",
        convertor  = NGS_PIPELINE + "/scripts/addAnnotations2vcf.pl"
    output:
        txt="{subject}/{base1}/calls/{base}.annotated.txt"
    version: "1.0"
    params:
        rulename   = "AttachAnnotation",
        batch      = config["job_addbackann"],
    shell: """
    #######################
    perl {input.convertor} {input.ref}  {input.txt} >{output.txt}
    #######################
    """
############
#       Expressed
############
rule Expressed:
    input: 
        RNASeq = lambda wildcards: expressedPairs[wildcards.sample],
        Mutation="{subject}/{sample}/calls/{base}.annotated.txt",
        convertor = NGS_PIPELINE + "/scripts/mpileup.pl"
    output: "{subject}/{sample}/calls/{base}.annotated.expressed.txt"
    version: config["samtools"]
    params:
        rulename  = "Expressed",
        batch     = config["job_expressed"]
    shell: """
    #######################
    module load samtools/{version}
    perl {input.convertor} {input.Mutation} {input.RNASeq} > {output}
    #######################
    """
############
#       Database Input
############
rule DBinput:
    input:
        txtFiles=lambda wildcards: SUBJECT_ANNO[wildcards.subject][wildcards.group],
        convertor=NGS_PIPELINE + "/scripts/makeDBVariantFile.pl"
    output: "{subject}/Subject/db/{subject}.{group}"
    params:
        rulename  = "DBinput"
    shell: """
    #######################
    perl {input.convertor} {input.txtFiles} >{output}
    #######################
    """    
############
#       Actionable
############
rule Actionable_Somatic:
    input:
        somatic="{subject}/Subject/db/{subject}.somatic",
        convertor=NGS_PIPELINE + "/scripts/" + config["Actionable_mutation"],
        annotation="{subject}/annotation/{subject}.Annotations.coding.rare.txt",
        refFile= config["annovar_data"]+"hg19_SomaticActionableSites.txt",
        cgc    = config["annovar_data"]+"geneLists/CancerGeneCensus.v76.txt",
        annotate  = NGS_PIPELINE + "/scripts/addAnnotations2vcf.pl",
    output:
        somatic="{subject}/{ACT_DIR}/{subject}.somatic.actionable.txt",
    params:
        rulename  = "Actionable_Somatic",
        batch    = config['job_default']
    shell: """
    #######################
    perl {input.convertor} somatic  {input.refFile} {input.cgc} {input.somatic} {input.annotation} >{output.somatic}
    #######################
    """
############
#       Actionable
############
rule Actionable_Germline:
    input:
        germline="{subject}/Subject/db/{subject}.germline",
        convertor=NGS_PIPELINE + "/scripts/" + config["Actionable_mutation"],
        annotation="{subject}/annotation/{subject}.Annotations.coding.rare.txt",
        annotate  = NGS_PIPELINE + "/scripts/addAnnotations2vcf.pl",
        cancerGeneCensus = config["annovar_data"]+"geneLists/CGCensus_Hereditary.txt",
        hotspot= config["annovar_data"]+"hg19_SomaticActionableSites.txt",
        tsid   = config["annovar_data"]+"geneLists/TruSightInheritedDiseases.txt",
        jsw    = config["annovar_data"]+"geneLists/CancerGenes_JSW.txt",
        clt2   = config["annovar_data"]+"geneLists/ClinomicsTier2GeneList.txt",
        ghr    = config["annovar_data"]+"geneLists/Genetics_HumanRef.3.8.16.txt",
        cgc    = config["annovar_data"]+"geneLists/CancerGeneCensus.v76.txt",
        combine=NGS_PIPELINE + "/scripts/germlineOnly.pl"
    output:
        germline="{subject}/{ACT_DIR}/{subject}.germline.actionable.txt",
    params:
        rulename  = "Actionable_Germline",
        batch    = config['job_default']
    shell: """
    #######################
    if [ -e {wildcards.subject}/{wildcards.subject}/db/{wildcards.subject}.somatic ]
    then 
        perl {input.convertor} germline {wildcards.subject}/{wildcards.subject}/db/{wildcards.subject}.somatic {input.germline} {input.annotation} {input.cancerGeneCensus} {input.hotspot} {input.tsid} {input.jsw} {input.clt2} {input.ghr} > {output.germline}
    else
        touch {input.germline}.dummy
        perl {input.convertor} germline {input.germline}.dummy {input.germline} {input.annotation} {input.cancerGeneCensus} {input.hotspot} {input.tsid} {input.jsw} {input.clt2} {input.ghr} > {output.germline}.gl
        perl {input.convertor} somatic  {input.hotspot} {input.cgc} {input.germline} {input.annotation} >{output.germline}.som
        perl {input.combine} {output.germline}.gl {output.germline}.som  >{output.germline}
        rm -rf {output.germline}.gl {output.germline}.som {input.germline}.dummy
    fi
    #######################
    """
############
#       Actionable
############
rule Actionable_RNAseq:
    input:
        rnaseq="{subject}/Subject/db/{subject}.rnaseq",
        convertor=NGS_PIPELINE + "/scripts/" + config["Actionable_mutation"],
        annotation="{subject}/annotation/{subject}.Annotations.coding.rare.txt",
        annotate  = NGS_PIPELINE + "/scripts/addAnnotations2vcf.pl",
        cancerGeneCensus = config["annovar_data"]+"geneLists/CGCensus_Hereditary.txt",
        hotspot= config["annovar_data"]+"hg19_SomaticActionableSites.txt",
        tsid   = config["annovar_data"]+"geneLists/TruSightInheritedDiseases.txt",
        jsw    = config["annovar_data"]+"geneLists/CancerGenes_JSW.txt",
        clt2   = config["annovar_data"]+"geneLists/ClinomicsTier2GeneList.txt",
        ghr    = config["annovar_data"]+"geneLists/Genetics_HumanRef.3.8.16.txt",
        cgc    = config["annovar_data"]+"geneLists/CancerGeneCensus.v76.txt",
        combine=NGS_PIPELINE + "/scripts/germlineOnly.pl"
    output:
        rnaseq="{subject}/{ACT_DIR}/{subject}.rnaseq.actionable.txt",
    params:
        rulename  = "Actionable_RNAseq",
        batch    = config['job_default']
    shell: """
    #######################
    touch {input.rnaseq}.dummy
    perl {input.convertor} rnaseq {input.rnaseq}.dummy {input.rnaseq} {input.annotation} {input.cancerGeneCensus} {input.hotspot} {input.tsid} {input.jsw} {input.clt2} {input.ghr} > {output.rnaseq}
    perl {input.convertor} somatic  {input.hotspot} {input.cgc} {input.rnaseq} {input.annotation} >{output.rnaseq}.som
    perl {input.combine} {output.rnaseq}.gl {output.rnaseq}.som  >{output.rnaseq}
    rm -rf {output.rnaseq}.gl {output.rnaseq}.som {input.rnaseq}.dummy
    #######################
    """
############
#    **END**
############
